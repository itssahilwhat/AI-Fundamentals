{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-29T11:57:25.345322Z",
     "start_time": "2025-04-29T11:57:02.567050Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:03:57.797986Z",
     "start_time": "2025-04-29T12:03:53.867224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Prepare data\n",
    "X_train = trainset.data.reshape(trainset.data.shape[0], -1).astype(np.float32)\n",
    "y_train = np.array(trainset.targets)\n",
    "X_test = testset.data.reshape(testset.data.shape[0], -1).astype(np.float32)\n",
    "y_test = np.array(testset.targets)\n",
    "\n",
    "# Normalize data\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0\n",
    "\n",
    "# Add bias term\n",
    "X_train = np.hstack((X_train, np.ones((X_train.shape[0], 1))))\n",
    "X_test = np.hstack((X_test, np.ones((X_test.shape[0], 1))))\n",
    "\n",
    "# Convert to tensors with float32 dtype\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).long()\n",
    "X_test_tensor = torch.from_numpy(X_test).float()\n",
    "y_test_tensor = torch.from_numpy(y_test).long()"
   ],
   "id": "9acaf7eb0e593d4c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:03:57.809936Z",
     "start_time": "2025-04-29T12:03:57.804952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Linear classifier\n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ],
   "id": "d78634f3968b2023",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:04:00.256166Z",
     "start_time": "2025-04-29T12:04:00.244910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loss functions\n",
    "def svm_loss(scores, labels, delta=1.0):\n",
    "    batch_size = scores.size(0)\n",
    "    correct_class_scores = scores[torch.arange(batch_size), labels].view(-1, 1)\n",
    "    margins = torch.max(torch.zeros_like(scores), scores - correct_class_scores + delta)\n",
    "    margins[torch.arange(batch_size), labels] = 0\n",
    "    loss = torch.mean(torch.sum(margins, dim=1))\n",
    "    return loss\n",
    "\n",
    "def softmax_loss(scores, labels):\n",
    "    batch_size = scores.size(0)\n",
    "    scores = scores - torch.max(scores, dim=1, keepdim=True).values\n",
    "    exp_scores = torch.exp(scores)\n",
    "    prob = exp_scores / torch.sum(exp_scores, dim=1, keepdim=True)\n",
    "    loss = -torch.mean(torch.log(prob[torch.arange(batch_size), labels]))\n",
    "    return loss"
   ],
   "id": "6e4f8c2e2e028913",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:04:00.741989Z",
     "start_time": "2025-04-29T12:04:00.724276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Regularization techniques\n",
    "def l1_regularization(model, lambda_l1=0.001):\n",
    "    l1_loss = 0\n",
    "    for param in model.parameters():\n",
    "        l1_loss += torch.sum(torch.abs(param))\n",
    "    return lambda_l1 * l1_loss\n",
    "\n",
    "def l2_regularization(model, lambda_l2=0.001):\n",
    "    l2_loss = 0\n",
    "    for param in model.parameters():\n",
    "        l2_loss += torch.sum(param ** 2)\n",
    "    return lambda_l2 * l2_loss\n",
    "\n",
    "def elastic_net_regularization(model, lambda_l1=0.001, lambda_l2=0.001):\n",
    "    return l1_regularization(model, lambda_l1) + l2_regularization(model, lambda_l2)"
   ],
   "id": "7eb990ea4d4d0a34",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:04:01.336307Z",
     "start_time": "2025-04-29T12:04:01.329234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Optimization demonstration\n",
    "def train_model(model, optimizer, X_train, y_train, X_test, y_test, epochs=10, regularization=None):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        scores = model(X_train)\n",
    "        loss = softmax_loss(scores, y_train)\n",
    "\n",
    "        if regularization == 'l1':\n",
    "            loss += l1_regularization(model)\n",
    "        elif regularization == 'l2':\n",
    "            loss += l2_regularization(model)\n",
    "        elif regularization == 'elastic_net':\n",
    "            loss += elastic_net_regularization(model)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Learning rate decay\n",
    "        if isinstance(optimizer, torch.optim.SGD) or isinstance(optimizer, torch.optim.Adam):\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] *= 0.99\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    test_scores = model(X_test)\n",
    "    test_loss = softmax_loss(test_scores, y_test)\n",
    "    print(f\"\\nFinal Test Loss: {test_loss.item():.4f}\")"
   ],
   "id": "37de6e128ea7abfa",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:07:29.372112Z",
     "start_time": "2025-04-29T12:06:50.721945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Demonstrate different optimizers\n",
    "input_size = X_train_tensor.shape[1]\n",
    "num_classes = 10\n",
    "\n",
    "# Vanilla SGD\n",
    "model = LinearClassifier(input_size, num_classes)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "print(\"Training with Vanilla SGD:\")\n",
    "train_model(model, optimizer, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)\n",
    "\n",
    "# SGD with Momentum\n",
    "model = LinearClassifier(input_size, num_classes)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "print(\"\\nTraining with SGD + Momentum:\")\n",
    "train_model(model, optimizer, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)\n",
    "\n",
    "# SGD with Nesterov Momentum\n",
    "model = LinearClassifier(input_size, num_classes)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, nesterov=True)\n",
    "print(\"\\nTraining with SGD + Nesterov Momentum:\")\n",
    "train_model(model, optimizer, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)\n",
    "\n",
    "# RMSProp\n",
    "model = LinearClassifier(input_size, num_classes)\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
    "print(\"\\nTraining with RMSProp:\")\n",
    "train_model(model, optimizer, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Adam\n",
    "model = LinearClassifier(input_size, num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "print(\"\\nTraining with Adam:\")\n",
    "train_model(model, optimizer, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)\n",
    "\n",
    "# AdamW\n",
    "model = LinearClassifier(input_size, num_classes)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "print(\"\\nTraining with AdamW:\")\n",
    "train_model(model, optimizer, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)\n",
    "\n",
    "# AdaGrad\n",
    "model = LinearClassifier(input_size, num_classes)\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=0.001)\n",
    "print(\"\\nTraining with AdaGrad:\")\n",
    "train_model(model, optimizer, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)\n",
    "\n",
    "# L-BFGS\n",
    "model = LinearClassifier(input_size, num_classes)\n",
    "\n",
    "# L-BFGS requires a different training loop because it's a batch optimizer\n",
    "def train_model_lbfgs(model, optimizer, X_train, y_train, X_test, y_test, epochs=10, regularization=None):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        # Define closure for L-BFGS\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            scores = model(X_train)\n",
    "            loss = softmax_loss(scores, y_train)\n",
    "\n",
    "            if regularization == 'l1':\n",
    "                loss += l1_regularization(model)\n",
    "            elif regularization == 'l2':\n",
    "                loss += l2_regularization(model)\n",
    "            elif regularization == 'elastic_net':\n",
    "                loss += elastic_net_regularization(model)\n",
    "\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        loss = optimizer.step(closure)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    test_scores = model(X_test)\n",
    "    test_loss = softmax_loss(test_scores, y_test)\n",
    "    print(f\"\\nFinal Test Loss: {test_loss.item():.4f}\")\n",
    "\n",
    "optimizer = optim.LBFGS(model.parameters(), lr=0.001)\n",
    "print(\"\\nTraining with L-BFGS:\")\n",
    "train_model_lbfgs(model, optimizer, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)\n",
    "\n",
    "# L1 Regularization\n",
    "model = LinearClassifier(input_size, num_classes)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "print(\"\\nTraining with L1 Regularization:\")\n",
    "train_model(model, optimizer, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, regularization='l1')\n",
    "\n",
    "# L2 Regularization\n",
    "model = LinearClassifier(input_size, num_classes)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "print(\"\\nTraining with L2 Regularization:\")\n",
    "train_model(model, optimizer, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, regularization='l2')\n",
    "\n",
    "# Elastic Net Regularization\n",
    "model = LinearClassifier(input_size, num_classes)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "print(\"\\nTraining with Elastic Net Regularization:\")\n",
    "train_model(model, optimizer, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, regularization='elastic_net')"
   ],
   "id": "a9a4ebad844afe0c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Vanilla SGD:\n",
      "Epoch 1, Loss: 2.3303\n",
      "Epoch 2, Loss: 2.3237\n",
      "Epoch 3, Loss: 2.3180\n",
      "Epoch 4, Loss: 2.3130\n",
      "Epoch 5, Loss: 2.3086\n",
      "Epoch 6, Loss: 2.3047\n",
      "Epoch 7, Loss: 2.3012\n",
      "Epoch 8, Loss: 2.2981\n",
      "Epoch 9, Loss: 2.2954\n",
      "Epoch 10, Loss: 2.2929\n",
      "\n",
      "Final Test Loss: 2.2919\n",
      "\n",
      "Training with SGD + Momentum:\n",
      "Epoch 1, Loss: 2.3595\n",
      "Epoch 2, Loss: 2.3512\n",
      "Epoch 3, Loss: 2.3382\n",
      "Epoch 4, Loss: 2.3244\n",
      "Epoch 5, Loss: 2.3129\n",
      "Epoch 6, Loss: 2.3051\n",
      "Epoch 7, Loss: 2.3007\n",
      "Epoch 8, Loss: 2.2988\n",
      "Epoch 9, Loss: 2.2978\n",
      "Epoch 10, Loss: 2.2966\n",
      "\n",
      "Final Test Loss: 2.2956\n",
      "\n",
      "Training with SGD + Nesterov Momentum:\n",
      "Epoch 1, Loss: 2.4054\n",
      "Epoch 2, Loss: 2.3770\n",
      "Epoch 3, Loss: 2.3470\n",
      "Epoch 4, Loss: 2.3213\n",
      "Epoch 5, Loss: 2.3029\n",
      "Epoch 6, Loss: 2.2919\n",
      "Epoch 7, Loss: 2.2868\n",
      "Epoch 8, Loss: 2.2853\n",
      "Epoch 9, Loss: 2.2849\n",
      "Epoch 10, Loss: 2.2840\n",
      "\n",
      "Final Test Loss: 2.2826\n",
      "\n",
      "Training with RMSProp:\n",
      "Epoch 1, Loss: 2.3448\n",
      "Epoch 2, Loss: 9.3914\n",
      "Epoch 3, Loss: 15.4589\n",
      "Epoch 4, Loss: 15.5058\n",
      "Epoch 5, Loss: 16.7541\n",
      "Epoch 6, Loss: 16.2943\n",
      "Epoch 7, Loss: 11.9994\n",
      "Epoch 8, Loss: 10.5956\n",
      "Epoch 9, Loss: 8.5202\n",
      "Epoch 10, Loss: 4.5200\n",
      "\n",
      "Final Test Loss: 3.4941\n",
      "\n",
      "Training with Adam:\n",
      "Epoch 1, Loss: 2.3644\n",
      "Epoch 2, Loss: 2.7180\n",
      "Epoch 3, Loss: 2.4068\n",
      "Epoch 4, Loss: 2.3584\n",
      "Epoch 5, Loss: 2.3855\n",
      "Epoch 6, Loss: 2.3168\n",
      "Epoch 7, Loss: 2.2580\n",
      "Epoch 8, Loss: 2.2369\n",
      "Epoch 9, Loss: 2.2295\n",
      "Epoch 10, Loss: 2.1924\n",
      "\n",
      "Final Test Loss: 2.1646\n",
      "\n",
      "Training with AdamW:\n",
      "Epoch 1, Loss: 2.3394\n",
      "Epoch 2, Loss: 2.6676\n",
      "Epoch 3, Loss: 2.3075\n",
      "Epoch 4, Loss: 2.3354\n",
      "Epoch 5, Loss: 2.3810\n",
      "Epoch 6, Loss: 2.3113\n",
      "Epoch 7, Loss: 2.2269\n",
      "Epoch 8, Loss: 2.1729\n",
      "Epoch 9, Loss: 2.1849\n",
      "Epoch 10, Loss: 2.2030\n",
      "\n",
      "Final Test Loss: 2.1784\n",
      "\n",
      "Training with AdaGrad:\n",
      "Epoch 1, Loss: 2.3785\n",
      "Epoch 2, Loss: 2.8163\n",
      "Epoch 3, Loss: 2.3657\n",
      "Epoch 4, Loss: 2.2899\n",
      "Epoch 5, Loss: 2.2603\n",
      "Epoch 6, Loss: 2.2468\n",
      "Epoch 7, Loss: 2.2345\n",
      "Epoch 8, Loss: 2.2232\n",
      "Epoch 9, Loss: 2.2126\n",
      "Epoch 10, Loss: 2.2027\n",
      "\n",
      "Final Test Loss: 2.1949\n",
      "\n",
      "Training with L-BFGS:\n",
      "Epoch 1, Loss: 2.3350\n",
      "Epoch 2, Loss: 2.3284\n",
      "Epoch 3, Loss: 2.3140\n",
      "Epoch 4, Loss: 2.2967\n",
      "Epoch 5, Loss: 2.2797\n",
      "Epoch 6, Loss: 2.2623\n",
      "Epoch 7, Loss: 2.2447\n",
      "Epoch 8, Loss: 2.2265\n",
      "Epoch 9, Loss: 2.2076\n",
      "Epoch 10, Loss: 2.1910\n",
      "\n",
      "Final Test Loss: 2.1751\n",
      "\n",
      "Training with L1 Regularization:\n",
      "Epoch 1, Loss: 2.6453\n",
      "Epoch 2, Loss: 2.6379\n",
      "Epoch 3, Loss: 2.6317\n",
      "Epoch 4, Loss: 2.6263\n",
      "Epoch 5, Loss: 2.6217\n",
      "Epoch 6, Loss: 2.6176\n",
      "Epoch 7, Loss: 2.6141\n",
      "Epoch 8, Loss: 2.6110\n",
      "Epoch 9, Loss: 2.6082\n",
      "Epoch 10, Loss: 2.6057\n",
      "\n",
      "Final Test Loss: 2.3265\n",
      "\n",
      "Training with L2 Regularization:\n",
      "Epoch 1, Loss: 2.3743\n",
      "Epoch 2, Loss: 2.3675\n",
      "Epoch 3, Loss: 2.3615\n",
      "Epoch 4, Loss: 2.3562\n",
      "Epoch 5, Loss: 2.3515\n",
      "Epoch 6, Loss: 2.3473\n",
      "Epoch 7, Loss: 2.3435\n",
      "Epoch 8, Loss: 2.3402\n",
      "Epoch 9, Loss: 2.3372\n",
      "Epoch 10, Loss: 2.3345\n",
      "\n",
      "Final Test Loss: 2.3273\n",
      "\n",
      "Training with Elastic Net Regularization:\n",
      "Epoch 1, Loss: 2.6591\n",
      "Epoch 2, Loss: 2.6484\n",
      "Epoch 3, Loss: 2.6391\n",
      "Epoch 4, Loss: 2.6309\n",
      "Epoch 5, Loss: 2.6237\n",
      "Epoch 6, Loss: 2.6174\n",
      "Epoch 7, Loss: 2.6118\n",
      "Epoch 8, Loss: 2.6068\n",
      "Epoch 9, Loss: 2.6024\n",
      "Epoch 10, Loss: 2.5985\n",
      "\n",
      "Final Test Loss: 2.3153\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1bf5c7b963157fac"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
